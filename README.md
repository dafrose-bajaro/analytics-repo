# analytics-repo

This is a Github repository template for analytics use cases. It uses data from [Project CCHAIN](https://thinkingmachines.github.io/project-cchain/), [NASA FIRMS](https://firms.modaps.eosdis.nasa.gov/), and [AQICN](https://aqicn.org/city/chiang-mai/). It also uses data engineering configurations from Kenneth Domingo's [analytics-data-eng-play respository](https://github.com/kvdomingo/analytics-data-engg-play/tree/main).

---

## Sample Use Cases <br><br>

1) Linking precipitation and dengue cases in selected Philippine cities. Project CCHAIN datasets are in a GCS bucket (.csv) and in Google BigQuery. <br><br>

2) Linking air quality index and fires in Chiang Mai. Air quality indices come from AQICN, while fire datasets come from NASA FIRMS, both of which are accessible via API. <br><br>

---

## Tech stack <br><br>
- [Python 3.12](https://docs.python.org/3.12/)
- [uv](https://docs.astral.sh/uv)
- [Dagster](https://docs.dagster.io)
- [Polars](https://docs.pola.rs)
- [DuckDB](https://duckdb.org/docs/stable/)
- [dbt](https://docs.getdbt.com/)

## Prerequisites <br><br>
- [Mise](https://mise.jdx.dev/getting-started.html)
- [Docker](https://docker.com)

---

## Key files<br><br>

- **dagster.yaml** configures a Dagster instance by defining settings for storage, run execution, logging, sensors, and schedulers. <br><br>

- **dbt_project.yaml** configures a dbt project by defining settings like project name, version, paths, models, and other configurations. <br><br>

- **docker-compose.yaml** defines Dagster as the main service for managing data workflows, PostgreSQL for storage, and configures environment variables. <br><br>

- **Dockerfile** creates a Docker image to prepare the environment, install dependencies, and set configurations for the workflow to function. <br><br>

- **.env.example** contains a template for keys and credentials. For this repo, it contains keys and credential to Google Cloud Storage where the datasets are stored. Create an actual **.env** file containing actual keys and credentials. It is worth noting that .env files should be included in gitignore so it does not get uploaded to the remote repository. <br><br>

- **.gitattributes** defines attributes and behaviors for specific file types and paths. For this specific repository, we ask that Jupyter notebooks (.json) be read using Python syntax. <br><br>

- **.gitignore** defines files to exclude from Git tracking. It uses a Python template and ignores Windows files and the data folder. <br><br>

- **.mise.toml** manages development environment and dependencies. <br><br>

- **.pre-commit-config.yaml** automates linting, formatting, and validation. It includes the following repos: (a) ruff-pre-commit, (b) sqlfluff, (c) jupytext, and (d) pre-commit hooks. <br><br>

- **profiles.yaml** defines how dbt connects to the database. The database type is DuckDB and it allocates 16 thread for parallel processing. <br><br>

- **pyproject.toml** contains dependencies, build system, and tool configurations (based on standard package config patterns). The `hatchling` build system is a common Python build system that works well with Dagster and pyproject. <br><br>

- **README.md** provides an overview of the repository: its purpose, structure, and instructions for setup. <br><br>

- **taskfile.yaml** automates tasks for container management and project workflows. Each task corresponds to a specific command that can be excuted (i.e., docker commands to initialize, setup, etc.). <br><br>

- All other files are autogenerated.

---


## Key folders <br><br>

- **data** stores the datasets. Git tracking ignores this folder as this mainly includes large downloaded files. `task init` makes a directory for `../data/lake`. <br><br>

- **docs** contains additional files documenting details for specific files, folders, functions, and processes. <br><br>

- **notebooks** contains Jupyter notebooks that can be run for downloading data, doing EDAs, and visualization. Categories under this folder include: (a) download, (b) processing, (c) analysis, and (d) visualization. <br><br>

- **src** contains Python scripts for reusable components (e.g., functions, imports, paths) used in the pipeline. <br><br>

- **tests** contains test scripts to validate the workflow. <br><br>

---

## What is in `src`?

The `src` folder is categorized into the following files and folders. <br><br>

### Folders

| Folder       | Role       | Example |
|--------------|---------------------------------|---------------------------------------------|
| **assets**   | Defines Dagster **assets**, which manage data ingestion, transformation, and storage. | Functions to fetch, store, and process data from APIs or cloud storage using I/O managers. |
| **internal** | Contains **helper functions and reusable utilities** that support asset execution. | Data cleaning, metadata generation, transformation logic, and dependency handling. |
| **schemas**  | Defines **structured data formats** to enforce consistency across Dagster assets. | `polars` schemas specifying column types, constraints, and validation rules for datasets. |

### Files

| File              | Role  |
|------------------|-----------------------------------------------|
| **dbt_project.py** | Defines how **dbt** integrates with Dagster, handling database transformations and workflow execution. |
| **definitions.py** | Acts as the **central Dagster configuration file**, integrating assets, resources, and jobs for pipeline execution. |
| **partitions.py** | Manages **time-based partitions** for Dagster assets, ensuring data is processed in segmented units (e.g., daily batching). |
| **resources.py** | Configures **external dependencies** (e.g., S3, DuckDB, DBT) using Dagster **resources and I/O managers** for data handling. |
| **settings.py** | Stores **global configuration settings** like credentials, time zones, and default variables for Dagster workflows. |

---


## Instructions for use <br><br>

1. Make sure that Docker is integrated to your OS. In Docker Desktop: Settings > Resources > WSL Integration > Select applicable. To check if Docker is running, run `docker --version`.
    ```shell
    docker --version
    ```
<br>

2. Install requirements in shell. **No need to run if these are already present in your system.** Note that repeating installs for `curl` will install multiple versions in your local.
    ```shell
    curl https://mise.run | sh #mise
    sudo snap install task #task
    ```
<br>

3. Make sure `mise` is activated and running properly by running `mise doctor`. If `mise` is not activated, run `eval "$(mise activate bash)"`.
    ```shell
    mise doctor
    eval "$(mise activate bash)"
    ```
<br>


4. Install tools in shell. See [.mise.toml](./.mise.toml) for a list of tools. `mise trust -y` checks for untrusted configuration tools. A `mise WARN` message stating "No untrusted config files found" means that config files are already trusted and no updates are needed.
    ```shell
    mise trust -y
    mise install -y
    ```
<br>

5. Run initial [setup script](./Taskfile.yml#L8) (i.e., `init` in `taskfile.yaml`). This will install [pre-commit hooks](./.pre-commit-config.yaml) for the project. It automates creation of the following files/folders: (a) `.python-version`, (b) `main.py`, (c) `uv.lock`, and (d) `/data/lake`.
    ```shell
    task init
    ```
    Note that you may need to run `uv init` if you are running a blank project. The task only syncs `uv` with the existing `pyproject.toml`.
<br>

6. Create a `.env` file using the template `.env.example`. <br><br>

7. Login to GCloud.
    ```shell
    gcloud auth login --update-adc
    ```
<br><br>

8. Build and start the [Docker container](./Taskfile.yml#L16=8) (i.e., `up` in `taskfile.yaml`). This will also setup cache folders for initialization and log files, linters, the virtual environment, and build artifacts.
    ```shell
    task up
    ```
<br>

9. Access the Dagster UI at `http://localhost:3030`. Both Dagster and Dagster-DB should be running and healthy. It takes a while to setup, but you can watch logs using the command below.
    ```shell
    task logs
    ```
<br>

10. To stop the running container, press `Ctrl+C` and stop the container.
    ```shell
    task down
    ```
<br>
